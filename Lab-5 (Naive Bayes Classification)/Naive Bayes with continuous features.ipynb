{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16c27c58",
   "metadata": {},
   "source": [
    "## Gaussian Naive Bayes classifier\n",
    "\n",
    "A Gaussian Naive Bayes algorithm is a special type of NB algorithm, used in case of continuous features. It assumes that all the features are following a Gaussian distribution i.e, normal distribution.\n",
    "\n",
    "There are two ways modifying continuous data: \n",
    "\n",
    "1. To fit an approximate distribution upon these features, most commonly the Gaussian (Normal) distribution. This is called Gaussian Naive Bayes.\n",
    "\n",
    "2. Convert continuous to categorical features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ae9d0e",
   "metadata": {},
   "source": [
    "## Step wise Implementatio\n",
    "\n",
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae730e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b51cd5",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aab4d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Breast_cancer_data.csv\")\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23404af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot a histogram to see target value distribution\n",
    "\n",
    "data[\"diagnosis\"].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61539fde",
   "metadata": {},
   "source": [
    "### Checking wheather the features are independent using a heat map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a915c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using pearson correlation\n",
    "\n",
    "corr = data.iloc[:,:-1].corr(method=\"pearson\")\n",
    "cmap = sns.diverging_palette(250,354,80,60,center='dark',as_cmap=True)\n",
    "sns.heatmap(corr, vmax=1, vmin=-.5, cmap=cmap, square=True, linewidths=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3b2da4",
   "metadata": {},
   "source": [
    "The attribute \"mean_parameter\" and \"mean_area\" are positiviely correlated to the attribute \"mean_radius\". However, there is no such correlation among the attributes \"mean_radius\", \"mean_texture\" and \"mean_smoothness\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29031175",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[[\"mean_radius\", \"mean_texture\", \"mean_smoothness\", \"diagnosis\"]]\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d782819c",
   "metadata": {},
   "source": [
    "### Approach 1: Fitting an approximate distribution upon these features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8517f685",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
    "sns.histplot(data, ax=axes[0], x=\"mean_radius\", kde=True, color='r')\n",
    "sns.histplot(data, ax=axes[1], x=\"mean_smoothness\", kde=True, color='b')\n",
    "sns.histplot(data, ax=axes[2], x=\"mean_texture\", kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7de77c3",
   "metadata": {},
   "source": [
    "All the three attributes kind of follow the normal distribution, so we can use the guassian distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64db214",
   "metadata": {},
   "source": [
    "### Calculating prior probabilities: P(Y=y) for all possible y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea209cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_prior(df, Y):\n",
    "    classes = sorted(list(df[Y].unique()))\n",
    "    prior = []\n",
    "    for i in classes:\n",
    "        prior.append(len(df[df[Y]==i])/len(df))\n",
    "    return prior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bae41c3",
   "metadata": {},
   "source": [
    "Calculating P(X=x|Y=y) using the Gaussian distribution. As per Naive bayes each feature is considered independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb44a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_likelihood_gaussian(df, feat_name, feat_val, Y, label):\n",
    "    feat = list(df.columns)\n",
    "    df = df[df[Y]==label]\n",
    "    mean, std = df[feat_name].mean(), df[feat_name].std() #calculating the mean and std for features\n",
    "    p_x_given_y = (1 / (np.sqrt(2 * np.pi) * std)) *  np.exp(-((feat_val-mean)**2 / (2 * std**2 ))) #pdf for normal distribution\n",
    "    return p_x_given_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269f2e40",
   "metadata": {},
   "source": [
    "### Calculating Posterior Probabilities: P(X=x1|Y=y)P(X=x2|Y=y)...P(X=xn|Y=y) * P(Y=y) for all y and find the maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca46897d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_gaussian(df, X, Y):\n",
    "    # get feature names\n",
    "    features = list(df.columns)[:-1]\n",
    "\n",
    "    # calculate prior for each class\n",
    "    prior = calculate_prior(df, Y)\n",
    "\n",
    "    Y_pred = []\n",
    "    # loop over every data sample\n",
    "    for x in X:   #iterate over all the data\n",
    "        # calculate likelihood\n",
    "        labels = sorted(list(df[Y].unique()))\n",
    "        likelihood = [1]*len(labels)\n",
    "        for j in range(len(labels)): #traverse through every class\n",
    "            for i in range(len(features)): ##for each feature\n",
    "                likelihood[j] *= calculate_likelihood_gaussian(df, features[i], x[i], Y, labels[j])\n",
    "\n",
    "        # calculate posterior probability (numerator only)\n",
    "        post_prob = [1]*len(labels)\n",
    "        for j in range(len(labels)):\n",
    "            post_prob[j] = likelihood[j] * prior[j]\n",
    "\n",
    "        Y_pred.append(np.argmax(post_prob))\n",
    "\n",
    "    return np.array(Y_pred) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f9006f",
   "metadata": {},
   "source": [
    "### Testing the Gaussian model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285cc537",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(data, test_size=.2, random_state=41)\n",
    "\n",
    "X_test = test.iloc[:,:-1].values\n",
    "Y_test = test.iloc[:,-1].values\n",
    "Y_pred = naive_bayes_gaussian(train, X=X_test, Y=\"diagnosis\")\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "print(confusion_matrix(Y_test, Y_pred))\n",
    "print(f1_score(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f55447",
   "metadata": {},
   "source": [
    "### Approach 2: Convert continuous features to Categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653dad63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dividing each features into three bins\n",
    "\n",
    "data[\"cat_mean_radius\"] = pd.cut(data[\"mean_radius\"].values, bins = 3, labels = [0,1,2])\n",
    "data[\"cat_mean_texture\"] = pd.cut(data[\"mean_texture\"].values, bins = 3, labels = [0,1,2])\n",
    "data[\"cat_mean_smoothness\"] = pd.cut(data[\"mean_smoothness\"].values, bins = 3, labels = [0,1,2])\n",
    "\n",
    "data = data.drop(columns=[\"mean_radius\", \"mean_texture\", \"mean_smoothness\"])\n",
    "data = data[[\"cat_mean_radius\",\t\"cat_mean_texture\",\t\"cat_mean_smoothness\", \"diagnosis\"]]\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbdb682",
   "metadata": {},
   "source": [
    "### Calculating P(X=x|Y=y) categorically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17b283a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_likelihood_categorical(df, feat_name, feat_val, Y, label):\n",
    "    feat = list(df.columns)\n",
    "    df = df[df[Y]==label]\n",
    "    p_x_given_y = len(df[df[feat_name]==feat_val]) / len(df)\n",
    "    return p_x_given_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc31b4a0",
   "metadata": {},
   "source": [
    "### Calculating P(X=x1|Y=y)P(X=x2|Y=y)...P(X=xn|Y=y) * P(Y=y) for all y and find the maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763d838b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_categorical(df, X, Y):\n",
    "    # get feature names\n",
    "    features = list(df.columns)[:-1]\n",
    "\n",
    "    # calculate prior\n",
    "    prior = calculate_prior(df, Y)\n",
    "\n",
    "    Y_pred = []\n",
    "    # loop over every data sample\n",
    "    for x in X:\n",
    "        # calculate likelihood\n",
    "        labels = sorted(list(df[Y].unique()))\n",
    "        likelihood = [1]*len(labels)\n",
    "        for j in range(len(labels)):\n",
    "            for i in range(len(features)):\n",
    "                likelihood[j] *= calculate_likelihood_categorical(df, features[i], x[i], Y, labels[j])\n",
    "\n",
    "        # calculate posterior probability (numerator only)\n",
    "        post_prob = [1]*len(labels)\n",
    "        for j in range(len(labels)):\n",
    "            post_prob[j] = likelihood[j] * prior[j]\n",
    "\n",
    "        Y_pred.append(np.argmax(post_prob))\n",
    "\n",
    "    return np.array(Y_pred) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17015923",
   "metadata": {},
   "source": [
    "### Testing the categorical model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d42bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(data, test_size=.2, random_state=41)\n",
    "\n",
    "X_test = test.iloc[:,:-1].values\n",
    "Y_test = test.iloc[:,-1].values\n",
    "Y_pred = naive_bayes_categorical(train, X=X_test, Y=\"diagnosis\")\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "print(confusion_matrix(Y_test, Y_pred))\n",
    "print(f1_score(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4d0290",
   "metadata": {},
   "source": [
    "## Evaluation Section:\n",
    "\n",
    "### Built a Naive bayes classifier from scratch to classify whether someone will attempt to evade taxes based on the following features:\n",
    "\n",
    "#### Refund: Categorical feature (Yes/No)\n",
    "#### Marital Status: Categorical feature (Single/Married/Divorced)\n",
    "#### Taxable Income: Continuous feature\n",
    "#### Evade: Target class (Yes/No)\n",
    "\n",
    "#### Assume categorical features follow categorical distribution, and continuous features follow a Gaussian distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141b9fc3",
   "metadata": {},
   "source": [
    "### Steps for Implementation:\n",
    "\n",
    "#### 1. Prepare the dataset\n",
    "#### 2. Calculate prior probabilities for each class (Evade = Yes/No)\n",
    "#### 3. Calculate likelihoods for categorical features 𝑃(𝑋𝑖∣𝑌)P(Xi∣Y)\n",
    "#### 4. Calculate likelihoods for continuous features 𝑃(𝑋𝑖∣𝑌)P(Xi∣Y) using the Gaussian distribution\n",
    "#### 5. Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665c0650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import sqrt, pi, exp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f70e7f8",
   "metadata": {},
   "source": [
    "### 1. Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0265b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'Refund': ['Yes', 'No', 'No', 'Yes', 'No', 'No', 'Yes', 'No', 'Yes', 'No'],\n",
    "    'Marital Status': ['Single', 'Married', 'Single', 'Married', 'Divorced', 'Married', 'Divorced', 'Single', 'Single', 'Married'],\n",
    "    'Taxable Income': [125000, 100000, 70000, 120000, 95000, 60000, 130000, 75000, 115000, 90000],\n",
    "    'Evade': ['No', 'No', 'No', 'No', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes']\n",
    "}\n",
    "\n",
    "# Convert to a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14824218",
   "metadata": {},
   "source": [
    "### 2. Calculatating Prior Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b23ae6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e45a9fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f535466",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
